概念总结

分类和回归属于监督学习，分类主要用于预测标称型数据，回归主要用于预测数值型数据。
监督学习指算法知道预测什么，对应的无监督学习处理的数据没有类别信息，也不会给定
目标值，将数据集合分成由类似的对象组成的多个类的过程称为聚类。
处理数据之前需要理解数据特征：特征值是离散型变量还是连续性变量，特征值中是否存
在缺失的值，何种原因造成缺失值，数据中是否存在异常值，某个特征发生的频率如何等。
开发过程：收集数据，准备输入数据，分析输入数据，训练算法（无监督学习不需要）。

+ kNN近邻算法：
         精度高、对异常值不敏感、无数据输入假定，但计算复杂度高、空间复杂度
         高，可应用在数值型和标称型。该算法通过计算目标数据到训练数据的距离，
         根据距离排序，选择排名靠前的几个训练数据类型，投票决定预测类型。
+ 决策树：
         计算复杂度不高、输出结果容易理解、对中间值缺失不敏感、可处理不相关特征，
         但可能会产生过度匹配问题，适用于数值型和标称型。该算法每次选择一个特征
         进行划分，依照该特征划分可以获得最大的信息增益，划分后将该特征移除，
         一直到所有训练数据均已分类成功或所有特征均已经使用。
+ 朴素贝叶斯：
         在数据较少的情况下依然有效、可以处理多类别问题，但对于输入数据的准备方
         式较为敏感，适用于标称型数据。该算法通过对目标数据计算概率，选择使概率
         最大的分类。算法需要一个先验输入，例如邮件分类系统中，需要对此前收到的
         邮件进行统计，计算垃圾邮件占比。
+ Logistic回归：
         计算代价不高，易于理解和实现，但容易欠拟合，分类精度可能不高，适用于数
         值型和标称型。该算法使用海维塞德阶跃函数进行二值分类，训练算法的过程就
         是在寻找回归系数，分类时用回归系数和输入向量的点乘计算阶跃函数的参数。
         寻找回归系数的过程可以使用梯度上升法，选择移动量最大的方向来迭代更新系数。
+ 支持向量机：
         泛化错误率低、计算开销不大、结果易解释，但对参数调节和核函数的选择敏感，
         院士分类器不加修改仅适用于处理二类问题。适用于数值型和标称型数据。其中
         一种实现是通过序列最小优化（SMO）算法，可以通过核函数将SVM拓展到无法线
         性划分的数据集。SVM的过程主要是寻找最佳分类间隔，这是一个线性平面，其最
         小间隔最大。核函数可以把低维空间数据映射到无穷维度，因此在当前空间无法
         线性划分的数据在无穷维度可以线性划分。
+ AdaBoost元算法：
         元算法是对其他算法进行组合的一种方式，其泛化错误率低，易编码、可以应用在
         大部分分类器上，对离群点敏感。适用于数值型和标称型数据。该算法使用多个弱
         分类器如单层决策树（某个特征基于阈值分类），每个弱分类器的权值不同，这个
         权值随着迭代次数增加不断更新。同时，每个样本也具有权重，如果某个样本被错
         误分类，则下次该样本的权重增加。最终对每个弱分类器的分类结果加权作为最终
         分类结果。
+ 回归： 
         回归结果易于理解、计算不复杂，但对非线性的数据拟合不好。适用于数值型和标
         称型。标准线性回归可以直接用矩阵计算，局部加权线性回归减少欠拟合现象，为
         数据点附近的其他数据点赋予不同权重。当数据的矩阵非奇异时，可以使用岭回归
         或者与lasso类似的前向逐步回归来约束回归系数。回归中的偏差指回归结果和真实
         结果的误差，而方差则指多次回归后不同回归系数之间的差异。
+ 树回归：
         可以对复杂和非线性的数据建模，但结果不易理解。适用于数值型和标称型数据。
         回归树的叶节点包含单个值，而模型树的叶节点包含一个线性方程。剪枝可以降
         低决策树的复杂度，包含预剪枝和后剪枝。
+ K-均值聚类：
         无监督学习，可以随即初始化k个簇，或者采用二分K-均值聚类算法，从一个簇开始
         划分。
+ Apriori算法:
         算法基于Apriori原则，如果某个项集不频繁，则其超集必然不频繁。可以从频繁
         相集中挖掘关联规则，如果某条规则不满足最小可信度要求，那么该规则的所有
         子集也不满足最小可信度要求。
+ FP-growth算法：
         算法速度快，只需要扫描整个数据集两次，但实现比较困难，在某些数据集上效率
         可能下降。适用于标称型数据。FP树的构造有些类似字典树，但增加了一个
         headerTable用于存储每个元素在树中首次出现的位置，并且为每个节点增加了parent域，
         从而可以从叶节点向上追溯，创建条件树，发现频繁项集。
+ PCA：
         PCA可以降低数据的复杂性，识别最重要的多个特征。但该步骤不一定重要，有可能
         损失有用信息。适用于数值型数据。另外几种降维技术有FA和ICA。算法通过协方差
         矩阵来计算主成分，并作为主要坐标轴，并继续计算次成分。当前几个成分覆盖了
         大部分方差时，可以认为后面的特征都是噪声。
+ SVD：
         SVD是一种矩阵分解技术，将矩阵Data分解成U、∑和V^T三个矩阵，其中∑矩阵为对角
         矩阵，并且从左到右数据逐渐减小，在r个数据后认为0。可以用这r个数据重构原矩
         阵的近似矩阵，计算方法为U[:r]*∑[:r]*VT[r:]。
+ MapReduce框架：
         当数据运算需求超过当前资源的运算能力，可以考虑借助MapReduce框架并行计算，
         利用网络服务提供的租赁资源。大部分情况下并不需要MapReduce。
