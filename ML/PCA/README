Dimensionality reduction techniques allow us to make data easier to use and often
remove noise to make other machine learning tasks more accurate. It's often a 
preprocessing step that can be done to clean up data before applying it to some
other algorithm. A number of techniques can be used to reduce the dimensionality 
of our data. Among these, independent component analysis, factor analysis, and
principal component analysis are popular methods. The most widely used method is
principle component analysis.

Principle component analysis allows the data to identify the important features. 
It does this by rotating the axes to align with the largest variance in the data.
Other axes are chosen orthogonal to the first axis in the direction of largest
variance. Eighenvalue analysis on the covariance matrix can be used to give us 
a set of orthogonal axes.

The PCA algorithm in this chapter loads the entire dataset into memory. If this 
isn't possible, other methods for finding the eigenvalues can be used. A good 
paper for an online method of finding the PCA is "Incremental Eigenanalysis for 
Classification". The singular value decomposition can also be used for eigenvalues 
analysis.

在低维下，数据更容易进行处理，其相关特征可能在数据中明确显示出来。PCA是降维技术
中最广泛的一种。降维技术使数据更易使用，并且它们往往能够去除数据中的噪声，通常
作为预处理步骤，在算法应用前清洗数据。PCA可以从数据中识别主要特征，它通过沿着
数据最大方差方向旋转坐标轴实现。如果要处理的数据过多无法放入内存，可以使用在线
PCA分析。
数据往往拥有超出显示能力的更多特征，简化数据不止使数据容易显示，同时降低算法开销、
去除噪声、使结果易懂。
主成分分析(Principal Component Analysis, PCA)将数据从原来的坐标第转移到新的坐标
系，新坐标系的选择由数据本身决定，新坐标系的第一个坐标轴是原始数据中方差最大的
方向，新坐标每户的第二个坐标轴和第一个坐标轴正交、并且最有最大方差。该过程一直
重复，次数为原始数据中维度。大部分方差都包含在前面几个新坐标轴中，因此可以忽略
剩下的坐标轴。
因子分析(Factor Analysis)假设观察数据的生成中有一些观察不到的隐变量，即观察数据
是由这些隐变量和某些噪声的线性组合，那么隐变量的数据可能比观察数据的数目少，找到
隐变量就可以实现数据的降维。
独立成分分析(Independent Component Analysis, ICA)假设数据从N个数据源生成，类似
因子分析，假设这些数据源之间在统计上相互独立，如果数据源数目少于观察数据数目，就
实现降维过程。

PCA可以降低数据复杂性，识别最重要的多个特征，但有时不一定需要，并且可能损失有用
信息。适用于数值型数据。

PCA实现过程：第一个主成分从数据差异性最大的方向获取，可以通过数据集的协方差矩阵
和特征值分析求得。
